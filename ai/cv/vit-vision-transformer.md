# ViT (Vision Transformer)

**"돋보기(작은 필터)로만 세상을 본다"**&#xB294; 그 구조 자체 ➡️ CNN의 결정적인 한계점

* Receptive Field(수용 영역)의 한계
* Inductive Bias(귀납적 편향)의 고정성 때문

***

#### 1. "나무만 보고 숲을 못 본다" (지역성 한계)

가장 큰 이유: CNN은 태생적으로 **"내 주변(Local)"**&#xB9CC; 볼 수 있다.

* CNN의 답답함:
  * 이미지 왼쪽 끝에 '강아지 얼굴'이 있고, 오른쪽 끝에 '강아지 꼬리'가 있는 상황
  * CNN의 돋보기(3 \* 3)는 이 둘을 한 번에 보지 못함
  * 층을 아주 많이 쌓아서(Deep), 위로 올라가고 합쳐져야 "아, 얼굴과 꼬리가 연결된 거구나"라고 뒤늦게 인지함
* ViT의 해결책 (Global Context):
  * Transformer의 Self-Attention은 거리 제한이없으니, 첫 번째 층에서 **'얼굴 패치'**&#xAC00; **'꼬리 패치'**&#xB97C; 바로 참조
  * **"이미지 전체의 문맥(Global Context)"**&#xC744; 처음부터 파악

> 비유:
>
> CNN: 퍼즐 조각을 하나씩 들고 "이건 파란색이네", "이건 빨간색이네" 하다가 나중에야 전체 그림을 아는 방식.
>
> ViT: 퍼즐판 전체를 쫙 펼쳐놓고, "이 조각은 저기 구석 조각이랑 색깔이 비슷하네?"라며 멀리 떨어진 관계를 바로 파악하는 방식.

#### 2. "중요한 것과 안 중요한 것을 똑같이 취급한다" (고정된 가중치)

CNN의 필터는 한 번 학습되면 이미지의 어느 부분을 보든 똑같은 방식으로 작동

* CNN의 답답함:
  * 이미지 속에 '복잡한 사람 얼굴'이 있든, '단순한 파란 하늘'이 있든 똑같은 필터로 똑같이 확인
  *   "하늘은 대충 보고 얼굴에 집중해!"라는 유동적인 대처에 약점

      &#x20;학습을 통해 얼굴에 반응하는 필터가 생기지만, 메커니즘 자체가 고정적
* ViT의 해결책 (Dynamic Attention):
  * Attention 메커니즘은 입력 데이터에 따라 가중치가 변화
  * "이 패치는 중요하니까 가중치를 0.9 주고, 저 패치는 배경이니까 0.1만 줘야지" 하는 식의 동적인(Dynamic) 집중이 가능

#### 3. "틀에 박힌 사고방식" (Inductive Bias의 역설)

CNN은 태어날 때부터 **"가까운 픽셀끼리는 친하다(Locality)"**&#xB77C;는 강력한 고정관념(Inductive Bias)을 가지고 설계

* CNN의 답답함:
  *   데이터가 적을 때는 고정관념이 정답을 빨리 찾는 가이드라인 역할

      ➡️ 그래서 적은 데이터로도 학습이 잘 됨
  *   하지만 데이터가 **수억 장(Big Data)**&#xC774; 넘어가면, 고정관념이 오히려 **성능의 천장(Limit)**

      ➡️ AI가 더 창의적으로 패턴을 찾을 수 있는데, "아니야, 가까운 것부터 봐!"라고 강요
* ViT의 해결책:
  * ViT는 고정관념이 없음 (백지상태)
  *   처음엔 멍청해 보이지만, 초대형 데이터를 제공하면, 편견 없이 데이터 자체의 패턴을 흡수

      ➡️ CNN의 성능을 뛰어넘

***

#### 요약: ViT가 등장한 이유

CNN은 **"부분을 합쳐 전체를 보는 방식"**&#xC774;라서, **"멀리 떨어진 정보들의 관계"**&#xB97C; 파악하는 데 비효율적

자연어 처리(NLP)에서 Transformer가 문장 전체의 단어 관계를 파악하는 걸 보고 아래와 같은 생각을 하게 됨

> "그림도 문장처럼 쪼개서 Transformer에 넣으면, CNN처럼 좁게 안 보고 전체를 시원하게 보지 않을까?"

➡️ 아이디어가 성공한 것이 바로 ViT

***

